"""
Module hu·∫•n luy·ªán m√¥ h√¨nh cho d·ª± √°n Customer Churn
√Åp d·ª•ng theo CRISP-DM Phase 4: Modeling v√† Phase 5: Evaluation
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix, 
    classification_report, roc_curve
)
import pickle
import matplotlib.pyplot as plt
import seaborn as sns


def train_logistic_regression(X_train, y_train, max_iter=1000):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression
    
    Args:
        X_train: D·ªØ li·ªáu hu·∫•n luy·ªán
        y_train: Nh√£n hu·∫•n luy·ªán
        max_iter: S·ªë v√≤ng l·∫∑p t·ªëi ƒëa
        
    Returns:
        model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
    """
    print("üîÑ ƒêang hu·∫•n luy·ªán Logistic Regression...")
    model = LogisticRegression(max_iter=max_iter, random_state=42)
    model.fit(X_train, y_train)
    print("‚úÖ Ho√†n t·∫•t hu·∫•n luy·ªán Logistic Regression")
    return model


def train_random_forest(X_train, y_train, n_estimators=100):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
    
    Args:
        X_train: D·ªØ li·ªáu hu·∫•n luy·ªán
        y_train: Nh√£n hu·∫•n luy·ªán
        n_estimators: S·ªë c√¢y trong r·ª´ng
        
    Returns:
        model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
    """
    print("üîÑ ƒêang hu·∫•n luy·ªán Random Forest...")
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    model.fit(X_train, y_train)
    print("‚úÖ Ho√†n t·∫•t hu·∫•n luy·ªán Random Forest")
    return model


def optimize_random_forest(X_train, y_train):
    """
    T·ªëi ∆∞u h√≥a si√™u tham s·ªë cho Random Forest b·∫±ng GridSearchCV
    
    Args:
        X_train: D·ªØ li·ªáu hu·∫•n luy·ªán
        y_train: Nh√£n hu·∫•n luy·ªán
        
    Returns:
        best_model: M√¥ h√¨nh t·ªët nh·∫•t sau t·ªëi ∆∞u
    """
    print("üîÑ ƒêang t·ªëi ∆∞u h√≥a Random Forest v·ªõi GridSearchCV...")
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    }
    
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(
        rf, param_grid, cv=5, 
        scoring='f1', n_jobs=-1, verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"‚úÖ Tham s·ªë t·ªët nh·∫•t: {grid_search.best_params_}")
    print(f"‚úÖ F1 Score t·ªët nh·∫•t (CV): {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_


def train_ensemble(X_train, y_train, lr_model, rf_model):
    """
    T·∫°o m√¥ h√¨nh Ensemble Voting t·ª´ Logistic Regression v√† Random Forest
    
    Args:
        X_train: D·ªØ li·ªáu hu·∫•n luy·ªán
        y_train: Nh√£n hu·∫•n luy·ªán
        lr_model: M√¥ h√¨nh Logistic Regression
        rf_model: M√¥ h√¨nh Random Forest
        
    Returns:
        ensemble_model: M√¥ h√¨nh Ensemble
    """
    print("üîÑ ƒêang t·∫°o Ensemble Voting Classifier...")
    
    ensemble = VotingClassifier(
        estimators=[('lr', lr_model), ('rf', rf_model)],
        voting='soft'
    )
    
    ensemble.fit(X_train, y_train)
    print("‚úÖ Ho√†n t·∫•t hu·∫•n luy·ªán Ensemble")
    
    return ensemble


def evaluate_model(model, X_test, y_test, model_name="Model"):
    """
    ƒê√°nh gi√° m√¥ h√¨nh v√† in ra c√°c metrics
    
    Args:
        model: M√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
        X_test: D·ªØ li·ªáu ki·ªÉm tra
        y_test: Nh√£n th·ª±c t·∫ø
        model_name: T√™n m√¥ h√¨nh
        
    Returns:
        dict: Dictionary ch·ª©a c√°c metrics
    """
    print(f"\nüìä ƒê√°nh gi√° {model_name}:")
    print("="*50)
    
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_proba)
    }
    
    print(f"Accuracy:  {metrics['accuracy']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall:    {metrics['recall']:.4f}")
    print(f"F1 Score:  {metrics['f1']:.4f}")
    print(f"AUC-ROC:   {metrics['auc']:.4f}")
    
    print("\nüìã Classification Report:")
    print(classification_report(y_test, y_pred, 
                                target_names=['No Churn', 'Churn']))
    
    return metrics


def compare_models(models_dict, X_test, y_test):
    """
    So s√°nh nhi·ªÅu m√¥ h√¨nh
    
    Args:
        models_dict: Dictionary {t√™n_m√¥_h√¨nh: m√¥_h√¨nh}
        X_test: D·ªØ li·ªáu ki·ªÉm tra
        y_test: Nh√£n th·ª±c t·∫ø
        
    Returns:
        pd.DataFrame: B·∫£ng so s√°nh metrics
    """
    results = []
    
    for name, model in models_dict.items():
        metrics = evaluate_model(model, X_test, y_test, name)
        metrics['model'] = name
        results.append(metrics)
    
    df_results = pd.DataFrame(results)
    df_results = df_results[['model', 'accuracy', 'precision', 'recall', 'f1', 'auc']]
    
    print("\n" + "="*70)
    print("üìä B·∫¢NG SO S√ÅNH M√î H√åNH")
    print("="*70)
    print(df_results.to_string(index=False))
    
    return df_results


def save_model(model, filepath):
    """
    L∆∞u m√¥ h√¨nh v√†o file .pkl
    
    Args:
        model: M√¥ h√¨nh c·∫ßn l∆∞u
        filepath: ƒê∆∞·ªùng d·∫´n file output
    """
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {filepath}")


def save_feature_columns(columns, filepath):
    """
    L∆∞u danh s√°ch t√™n c·ªôt ƒë·∫∑c tr∆∞ng
    
    Args:
        columns: Danh s√°ch t√™n c·ªôt
        filepath: ƒê∆∞·ªùng d·∫´n file output
    """
    with open(filepath, 'wb') as f:
        pickle.dump(columns, f)
    print(f"‚úÖ ƒê√£ l∆∞u feature columns t·∫°i: {filepath}")


def load_model(filepath):
    """
    T·∫£i m√¥ h√¨nh t·ª´ file .pkl
    
    Args:
        filepath: ƒê∆∞·ªùng d·∫´n file m√¥ h√¨nh
        
    Returns:
        model: M√¥ h√¨nh ƒë√£ t·∫£i
    """
    with open(filepath, 'rb') as f:
        model = pickle.load(f)
    print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh t·ª´: {filepath}")
    return model


if __name__ == "__main__":
    # Import preprocessing
    from preprocessing import preprocess_pipeline
    
    # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    filepath = "../WA_Fn-UseC_-Telco-Customer-Churn.csv"
    X_train, X_test, y_train, y_test, scaler = preprocess_pipeline(filepath)
    
    # Hu·∫•n luy·ªán c√°c m√¥ h√¨nh
    lr_model = train_logistic_regression(X_train, y_train)
    rf_model = train_random_forest(X_train, y_train)
    
    # T·ªëi ∆∞u Random Forest
    best_rf = optimize_random_forest(X_train, y_train)
    
    # Ensemble
    ensemble = train_ensemble(X_train, y_train, lr_model, best_rf)
    
    # So s√°nh
    models = {
        'Logistic Regression': lr_model,
        'Random Forest': rf_model,
        'Optimized Random Forest': best_rf,
        'Ensemble': ensemble
    }
    
    results_df = compare_models(models, X_test, y_test)
    
    # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
    save_model(best_rf, "../models/best_rf_model.pkl")
    save_model(scaler, "../models/scaler.pkl")
    save_feature_columns(X_train.columns.tolist(), "../models/feature_columns.pkl")
